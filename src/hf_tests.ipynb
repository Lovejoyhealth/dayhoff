{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirchar/miniconda3/envs/dayhoff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, Sequence, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP\n",
    ")\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sequence_models.samplers import (\n",
    "    SortishSampler,\n",
    "    ApproxBatchSampler,\n",
    "    ClusteredSortishSampler,\n",
    ")\n",
    "\n",
    "from evodiff.utils import Tokenizer\n",
    "from dayhoff.collators import LMCollator, OAMaskCollator\n",
    "from dayhoff.constants import MSA_ALPHABET_PLUS, TaskType\n",
    "from dayhoff.datasets import UniRefDataset\n",
    "from dayhoff.model import (\n",
    "    ARDiffusionModel,\n",
    "    OrderAgnosticDiffusionModel,\n",
    ")\n",
    "from dayhoff.model import create_model\n",
    "\n",
    "\n",
    "# default to a single-GPU setup if not present\n",
    "RANK = 0\n",
    "LOCAL_RANK = 0\n",
    "WORLD_SIZE = 1\n",
    "DEVICE = torch.device(f\"cuda:0\")\n",
    "\n",
    "\n",
    "def is_amlt() -> bool:\n",
    "    return os.environ.get(\"AMLT_OUTPUT_DIR\", None) is not None\n",
    "\n",
    "\n",
    "def load_config_and_model(\n",
    "    config_fpath: str,\n",
    ") -> Tuple[dict, Tokenizer, nn.Module, Type[nn.Module]]:\n",
    "    \"\"\"Parses the experiment config to load the model and tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config_fpath: str\n",
    "        The path to the experiment config file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    config: dict\n",
    "        The experiment config\n",
    "    tokenizer: Tokenizer\n",
    "        The model's tokenizer\n",
    "    model: nn.Module\n",
    "        A task-wrapped version of the specified model, which returns the appropriate loss and metrics\n",
    "    block: Type[nn.Module]\n",
    "        The block class used repeatedly in the module. It should not be split by any sharding.\n",
    "    \"\"\"\n",
    "    with open(config_fpath, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    config[\"task\"] = config[\"task\"].lower().strip()\n",
    "    tokenizer = Tokenizer(MSA_ALPHABET_PLUS)\n",
    "    task = TaskType(config[\"task\"].lower().strip())\n",
    "\n",
    "    # create the model\n",
    "    model, block = create_model(\n",
    "        task, config[\"model_type\"], config[\"model_config\"], tokenizer.mask_id.item()\n",
    "    )\n",
    "\n",
    "    # add the task-specific wrapper\n",
    "    aux_loss_weight = config.get(\"aux_loss_weight\", 0.0)\n",
    "    if task == TaskType.OADM:\n",
    "        model = OrderAgnosticDiffusionModel(\n",
    "            model, tokenizer.pad_id, aux_loss_weight=aux_loss_weight\n",
    "        )\n",
    "    elif task == TaskType.LM:\n",
    "        model = ARDiffusionModel(model, aux_loss_weight=aux_loss_weight)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {config['task']}\")\n",
    "    return config, tokenizer, model, block\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    config: dict, tokenizer: Tokenizer, args: argparse.Namespace\n",
    ") -> DataLoader:\n",
    "    if is_amlt():\n",
    "        data_top_dir = args.data_root or \"/ddn/evodiff/\"\n",
    "    else:\n",
    "        data_top_dir = args.data_root or \"/data1/data/\"\n",
    "\n",
    "    dataset = config[\"dataset\"]\n",
    "    data_dir = os.path.join(data_top_dir, dataset + \"/\")\n",
    "\n",
    "    if config[\"task\"] == \"oadm\":\n",
    "        collator = OAMaskCollator(\n",
    "            tokenizer=tokenizer,\n",
    "            pad_to_multiple_of=config.get(\"pad_to_multiple_of\", None),\n",
    "        )\n",
    "    elif config[\"task\"] == \"lm\":\n",
    "        collator = LMCollator(\n",
    "            tokenizer=tokenizer,\n",
    "            pad_to_multiple_of=config.get(\"pad_to_multiple_of\", None),\n",
    "            flip_prob=config.get(\"flip_prob\", 0.0),\n",
    "            fim_prob=config.get(\"fim_prob\", 0.0),\n",
    "            swap_bos_eos_on_flip=config.get(\"swap_bos_eos_on_flip\", True),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {config['task']}\")\n",
    "\n",
    "    # load the dataset\n",
    "    ds_train = UniRefDataset(data_dir, \"train\", max_len=config[\"max_len\"])\n",
    "    train_idx = ds_train.indices\n",
    "\n",
    "    # create the dataloader\n",
    "    if args.mini_run:\n",
    "        tindices = np.arange(\n",
    "            0, 1000\n",
    "        )  # np.arange(21546293,31546293,1)#(1000000,21546293, 1)\n",
    "        train_indices = np.sort(np.random.choice(tindices, 100, replace=False))\n",
    "        train_sampler = Subset(ds_train, train_indices)\n",
    "        len_train = train_indices\n",
    "        dl_train = DataLoader(\n",
    "            dataset=train_sampler,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "            num_workers=1,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    else:\n",
    "        metadata = np.load(os.path.join(data_dir, \"lengths_and_offsets.npz\"))\n",
    "        len_train = np.minimum(metadata[\"ells\"][train_idx], config[\"max_len\"])\n",
    "        if \"uniref50\" in dataset:\n",
    "            train_sortish_sampler = SortishSampler(\n",
    "                len_train, config[\"bucket_size\"], num_replicas=WORLD_SIZE, rank=RANK\n",
    "            )\n",
    "        elif \"uniref90\" in dataset:\n",
    "            with open(os.path.join(data_dir) + \"clustered_splits.json\") as f:\n",
    "                clusters = json.load(f)[\"train\"]\n",
    "            train_sortish_sampler = ClusteredSortishSampler(\n",
    "                len_train,\n",
    "                clusters,\n",
    "                config[\"bucket_size\"],\n",
    "                num_replicas=WORLD_SIZE,\n",
    "                rank=RANK,\n",
    "            )\n",
    "        train_sampler = ApproxBatchSampler(\n",
    "            train_sortish_sampler,\n",
    "            config[\"max_tokens\"],\n",
    "            config[\"max_batch_size\"],\n",
    "            len_train,\n",
    "            batch_mult=8,\n",
    "        )\n",
    "\n",
    "        dl_train = DataLoader(\n",
    "            dataset=ds_train,\n",
    "            batch_sampler=train_sampler,\n",
    "            num_workers=8,\n",
    "            collate_fn=collator,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    return dl_train\n",
    "\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_fpath\")\n",
    "parser.add_argument(\n",
    "    \"out_fpath\",\n",
    "    type=str,\n",
    "    nargs=\"?\",\n",
    "    default=os.getenv(\"AMLT_OUTPUT_DIR\", \"/tmp\") + \"/\",\n",
    ")\n",
    "parser.add_argument(\"data_root\", type=str, nargs=\"?\", default=None)\n",
    "parser.add_argument(\n",
    "    \"--mini_run\", action=\"store_true\"\n",
    ")  # Set to True if running on subset of data\n",
    "parser.add_argument(\"--checkpoint_freq\", type=int, default=2000)  # in steps\n",
    "parser.add_argument(\n",
    "    \"--random_seed\", type=int, default=0\n",
    ")  # lambda reweighting term from Austin D3PM\n",
    "parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")\n",
    "parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_wandb\", action=\"store_true\")\n",
    "parser.add_argument(\"--last_step\", default=-1, type=int)\n",
    "\n",
    "fake_args = [\n",
    "    \"jamba3B-uniref50.json\",       # config_fpath\n",
    "    \"/tmp/output/\",        # out_fpath\n",
    "    \"../data/\",  # data_root\n",
    "    # \"--mini_run\",\n",
    "    \"--no_wandb\"\n",
    "]\n",
    "\n",
    "args = parser.parse_args(fake_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job on rank 0 with local rank 0 and world size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirchar/miniconda3/envs/dayhoff/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 30 as padding index\n",
      "Using 28 as masking index\n",
      "Model has 2981431616 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Starting job on rank {RANK} with local rank {LOCAL_RANK} and world size {WORLD_SIZE}\"\n",
    ")\n",
    "seed_everything(args.random_seed)\n",
    "\n",
    "# dist.init_process_group(backend=\"nccl\")\n",
    "# get the config, tokenizer, and model\n",
    "if args.verbose:\n",
    "    print(\"Initializing model...\", RANK)\n",
    "config, tokenizer, model, blk_types = load_config_and_model(args.config_fpath)\n",
    "if RANK == 0:\n",
    "    if args.no_wandb:\n",
    "        wandbmode = \"disabled\"\n",
    "    else:\n",
    "        wandbmode = \"online\"\n",
    "    wandb.init(config=config, mode=wandbmode)\n",
    "if args.verbose:\n",
    "    print(\"Done initializing model.\", RANK)\n",
    "\n",
    "# store the command line args in the config and dump to disk\n",
    "config[\"dtype\"] = args.dtype\n",
    "config[\"random_seed\"] = args.random_seed\n",
    "config[\"world_size\"] = WORLD_SIZE\n",
    "if RANK == 0:\n",
    "    os.makedirs(args.out_fpath, exist_ok=True)\n",
    "    with open(os.path.join(args.out_fpath, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "# training dtype and local device\n",
    "dtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}[args.dtype]\n",
    "\n",
    "padding_idx = tokenizer.pad_id  # PROTEIN_ALPHABET.index(PAD)\n",
    "if RANK == 0:\n",
    "    print(\"Using {} as padding index\".format(padding_idx))\n",
    "    print(\"Using {} as masking index\".format(tokenizer.mask_id))\n",
    "    print(\n",
    "        f\"Model has {sum(p.numel() for p in model.parameters())} trainable parameters.\"\n",
    "    )\n",
    "if args.verbose:\n",
    "    print(\"Moving and sharding model...\", RANK)\n",
    "# set the default device\n",
    "torch.cuda.set_device(LOCAL_RANK)\n",
    "\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Initializing data...\", RANK)\n",
    "dl_train = get_dataloader(config, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter = iter(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter_obs = next(dl_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 80]), torch.Size([1000, 80]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_train_iter_obs[0].shape,dl_train_iter_obs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter_obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([4, 1, 3, 3]), 'text': [\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\", 'Got a letter in the mail last week that said Dr. Goldberg is moving to Arizona to take a new position there in June.  He will be missed very much.  \\\\n\\\\nI think finding a new doctor in NYC that you actually like might almost be as awful as trying to find a date!']}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dayhoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
