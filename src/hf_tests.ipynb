{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "import pyfastx\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Literal\n",
    "from multiprocessing import cpu_count\n",
    "import ijson\n",
    "from shutil import disk_usage\n",
    "\n",
    "def json_generator(json_path, key):\n",
    "    with open(json_path,'r') as f: \n",
    "        for record in ijson.items(f,f\"{key}.item\"):\n",
    "            yield  {\"ids\":record}\n",
    "            \n",
    "def parse_pyfastx_generator(fasta_fpath):\n",
    "    fasta = pyfastx.Fastx(fasta_fpath,comment=True) # Fasta fasta parser written in C\n",
    "    idx = 0\n",
    "    for accession, seq, description in fasta:\n",
    "        yield {\n",
    "            \"index\": idx,\n",
    "            \"sequence\": seq,\n",
    "            \"accession\": accession,\n",
    "            \"description\": description\n",
    "        }\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "def make_dset_from_ids(ids_dataset: Dataset, seq_dset: Dataset, num_proc: int = cpu_count()) -> Dataset:\n",
    "    # Using ids_dataset from a generator instead of from dict ensure map uses temp files in disk\n",
    "    # instead of loading everything in memory\n",
    "    return ids_dataset.map(lambda x: seq_dset[x[\"ids\"]],\n",
    "                      remove_columns=\"ids\",\n",
    "                      num_proc=num_proc)\n",
    "\n",
    "def create_hf_dataset(fasta_path: str,\n",
    "                      splits_path: str,\n",
    "                      dataset_type: Literal['clustered', 'unclustered'],\n",
    "                      num_proc: int = cpu_count()) -> DatasetDict:\n",
    "\n",
    "    ds = Dataset.from_generator(\n",
    "        parse_pyfastx_generator,\n",
    "        gen_kwargs={\n",
    "            \"fasta_fpath\": fasta_path\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if splits_path is None:\n",
    "        return ds\n",
    "\n",
    "    if dataset_type == 'unclustered':\n",
    "\n",
    "        with open(splits_path,'r') as f:  # load in memory. WARNING: May need to make it a DatasetDict like with clustered\n",
    "            splits = json.load(f)\n",
    "\n",
    "        ds_dict = DatasetDict({\n",
    "            split:ds.select(splits[split]) for split in splits\n",
    "        })\n",
    "        \n",
    "    elif dataset_type == 'clustered':\n",
    "        splits = ['train', 'test', 'valid', 'rtest']\n",
    "\n",
    "        \n",
    "        ids_dataset = DatasetDict(\n",
    "            {\n",
    "                split: Dataset.from_generator(\n",
    "                    json_generator,\n",
    "                    gen_kwargs={\"json_path\": splits_path, \"key\": split}\n",
    "                    ) for split in splits\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        ds_dict = make_dset_from_ids(ids_dataset = ids_dataset,\n",
    "                                     seq_dset = ds,\n",
    "                                     num_proc = num_proc)\n",
    "        \n",
    "            \n",
    "    return ds_dict\n",
    "\n",
    "def merge_and_create_hf_dataset(fasta_paths: list):\n",
    "    ds_dict = {}\n",
    "    for fasta_path in fasta_paths:\n",
    "        name = fasta_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        ds_dict[name] = Dataset.from_generator(\n",
    "                            parse_pyfastx_generator,\n",
    "                            gen_kwargs={\n",
    "                                \"fasta_fpath\": fasta_path\n",
    "                            }\n",
    "                        )\n",
    "    return DatasetDict(ds_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_large_files(directory,threshold=50*1024**3):\n",
    "    path = Path(directory)\n",
    "    for file in path.rglob('*'):\n",
    "        # print(file)\n",
    "        if file.is_file():\n",
    "            try:\n",
    "                if file.stat().st_size > threshold:\n",
    "                    print(f\"{file} is larger than 50GB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not get size for {file}: {e}\")\n",
    "\n",
    "find_large_files(\"../data/gigaref_full/\") # Check if any of the files are larger than 50GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_large_folder\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=\"\")\n",
    "# api.create_repo(repo_id='samirchar/testing', repo_type='dataset', private=True)\n",
    "# Check if repo already exists\n",
    "# repo_exists = api.repo_exists(repo_id=repo_id, repo_type=repo_type)\n",
    "upload_large_folder(folder_path = \"../data/rfdiffusion/\",\n",
    "                    repo_id = 'samirchar/testing',\n",
    "                    repo_type = 'dataset',\n",
    "                    multi_commits= True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/mnt/blob/hf_cache_test_not_writable/\", exist_ok=True,mode=0o555)\n",
    "os.chmod(\"/mnt/blob/hf_cache_test_not_writable/\", 0o555)\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/blob/hf_cache_test_not_writable/\"\n",
    "from datasets import Dataset\n",
    "from shutil import disk_usage\n",
    "def foo_gen():\n",
    "    for i in range(100):\n",
    "        yield {\"index\": i, \"sequence\": \"A\" * 100, \"accession\": f\"acc_{i}\", \"description\": f\"desc_{i}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dataset.from_generator(\n",
    "    foo_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.info.size_in_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\"../data/uniref90_202401/arrow/train\")\n",
    "ds.save_to_disk(\"/tmp_tests/uniref90_no_mp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/gigaref/no_singletons/clustered_splits.json','r') as f:\n",
    "    splits = json.dumps(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_generator(\n",
    "            json_generator,\n",
    "            gen_kwargs={\"json_path\": '../data/uniref50_202401/splits.json', \"key\": split}\n",
    "            ) for split in splits\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = '../data/uniref90_202401/clustered_splits.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set = Dataset.from_json('../data/uniref90_202401/clustered_splits.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set.cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def foo(sample_ids):\n",
    "    sample = []\n",
    "    for i in sample_ids:\n",
    "        seq = 'A'*500\n",
    "        sample.append({\"a\":seq,\"b\":i,\"c\":i})\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set.map(lambda x: foo(x[\"sample_ids\"]),\n",
    "                    remove_columns=\"sample_ids\",\n",
    "                    num_proc=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "\n",
    "set_seed(0)\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('samirchar/test_dayhoff', subfolder = \"jamba-170m-seqsam-36w\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('samirchar/test_dayhoff', trust_remote_code=True)\n",
    "\n",
    "\n",
    "inputs = tokenizer(tokenizer.bos_token, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "\n",
    "outputs = model.generate(inputs['input_ids'],max_length=50,do_sample=True)\n",
    "sequence = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offsets = np.load('../data/uniref50_202401/lengths_and_offsets.npz')\n",
    "import json\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os.path as osp\n",
    "import pyfastx\n",
    "from typing import Literal\n",
    "from multiprocessing import cpu_count\n",
    "def parse_pyfastx_generator(fasta_fpath):\n",
    "    fasta = pyfastx.Fastx(fasta_fpath,comment=True) # Fasta fasta parser written in C\n",
    "    idx = 0\n",
    "    for accession, seq, description in fasta:\n",
    "        yield {\n",
    "            \"index\": idx,\n",
    "            \"sequence\": seq,\n",
    "            \"accession\": accession,\n",
    "            \"description\": description\n",
    "        }\n",
    "        idx += 1\n",
    "\n",
    "def make_dset_from_ids(ids: list, seq_dset: Dataset, num_proc: int = cpu_count()) -> Dataset:\n",
    "    id_set = Dataset.from_dict({\"sample_ids\": ids})\n",
    "    return id_set.map(lambda x: seq_dset[x[\"sample_ids\"]],\n",
    "                      remove_columns=\"sample_ids\",\n",
    "                      num_proc=num_proc)\n",
    "\n",
    "\n",
    "def create_hf_dataset(fasta_path: str,\n",
    "                      splits_path: str,\n",
    "                      dataset_type: Literal['clustered', 'unclustered'],\n",
    "                      num_proc: int = cpu_count()) -> DatasetDict:\n",
    "\n",
    "    ds = Dataset.from_generator(\n",
    "        parse_pyfastx_generator,\n",
    "        gen_kwargs={\n",
    "            \"fasta_fpath\": fasta_path\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if splits_path is None:\n",
    "        return ds\n",
    "\n",
    "    with open(splits_path,'r') as f: \n",
    "        splits = json.load(f)\n",
    "\n",
    "\n",
    "    if dataset_type == 'unclustered':\n",
    "        ds = DatasetDict({\n",
    "            split:ds.select(splits[split]) for split in splits\n",
    "        })\n",
    "        \n",
    "    elif dataset_type == 'clustered':\n",
    "        ds = DatasetDict({\n",
    "            split:make_dset_from_ids(ids = splits[split],\n",
    "                                     seq_dset = ds,\n",
    "                                     num_proc = num_proc) for split in splits\n",
    "        })\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,i in enumerate(parse_pyfastx_generator('../data/rfdiffusion/rfdiffusion_both_filter.fasta')):\n",
    "    print(i)\n",
    "    if idx==10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE SAMPLE DATASETS ##\n",
    "\n",
    "sample_splits = {\"train\":[0,1,2,3,4],\n",
    "                 \"valid\":[5,6],\n",
    "                 \"test\":[7,8],\n",
    "                 \"rtest\":[9]}\n",
    "\n",
    "sample_clustered_splits = {\"train\":[[0,1],[2,3]],\n",
    "                 \"valid\":[[4],[5,6]],\n",
    "                 \"test\":[[7,8]],\n",
    "                 \"rtest\":[[9]]}\n",
    "\n",
    "with open('../data/uniref50_202401/sample_splits.json','w') as f:\n",
    "    json.dump(sample_splits,f)\n",
    "\n",
    "with open('../data/uniref90_202401/sample_clustered_splits.json','w') as f:\n",
    "    json.dump(sample_clustered_splits,f)\n",
    "\n",
    "fasta_dir = '../data/uniref50_202401/'\n",
    "\n",
    "ds = create_hf_dataset(fasta_path = osp.join(fasta_dir,'consensus_sample.fasta'),\n",
    "                        splits_path = osp.join(fasta_dir,'sample_splits.json'),\n",
    "                        dataset_type = 'unclustered'\n",
    "                        )\n",
    "\n",
    "ds.save_to_disk(osp.join(fasta_dir,'parquets'))\n",
    "\n",
    "\n",
    "ds = create_hf_dataset(fasta_path = osp.join('../data/uniref50_202401/','consensus_sample.fasta'),\n",
    "                        splits_path = osp.join('../data/uniref90_202401/','sample_clustered_splits.json'),\n",
    "                        dataset_type = 'clustered'\n",
    "                        )\n",
    "\n",
    "ds.save_to_disk(osp.join('../data/uniref90_202401/','parquets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os.path as osp\n",
    "import os\n",
    "from datasets import Dataset, Sequence\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "from huggingface_hub import hf_hub_url\n",
    "import numpy as np\n",
    "import json\n",
    "import pyfastx\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "Dayhoff dataset\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add a link to an official homepage for the dataset here\n",
    "_REPO_ID = \"samirchar/DayhoffDataset\"\n",
    "_HOMEPAGE = f\"https://huggingface.co/datasets/{_REPO_ID}\"\n",
    "\n",
    "# TODO: Add the licence for the dataset here if you can find it\n",
    "_LICENSE = \"\"\n",
    "\n",
    "#TODO: Add citation\n",
    "_CITATION = \"\"\n",
    "\n",
    "# TODO: Add link to the official dataset URLs here\n",
    "# The HuggingFace Datasets library doesn't host the datasets but only points to the original files.\n",
    "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\n",
    "_URLS = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def parse_pyfastx_generator(fasta_fpath):\n",
    "    fasta = pyfastx.Fastx(fasta_fpath,comment=True) # Fasta fasta parser written in C\n",
    "    idx = 0\n",
    "    for accession, seq, description in fasta:\n",
    "        yield {\n",
    "            \"index\": idx,\n",
    "            \"sequence\": seq,\n",
    "            \"accession\": accession,\n",
    "            \"description\": description\n",
    "        }\n",
    "        idx += 1\n",
    "\n",
    "def make_dset_from_ids(ids: list, seq_dset: Dataset, num_proc: int = cpu_count()) -> Dataset:\n",
    "    id_set = Dataset.from_dict({\"sample_ids\": ids})\n",
    "    return id_set.map(lambda x: seq_dset[x[\"sample_ids\"]],\n",
    "                      remove_columns=\"sample_ids\",\n",
    "                      num_proc=num_proc)\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class ClusteredSequencesConfig(datasets.BuilderConfig):\n",
    "        '''Congif for sequence generation'''\n",
    "        name: str = \"clustered\"\n",
    "        dataset: Literal[\"uniref90_202401\",\n",
    "                         \"gigaref\"] = \"uniref90_202401\"\n",
    "\n",
    "@dataclass\n",
    "class SequencesConfig(datasets.BuilderConfig):\n",
    "        '''Congif for sequence generation'''\n",
    "        name: str = \"sequence\"\n",
    "        dataset: Literal[\"uniref50_202401\",\n",
    "                         \"uniref90_202401\",\n",
    "                         \"rfdiffusion_both_filter\",\n",
    "                         \"rfdiffusion_novelty\",\n",
    "                         \"rfdiffusion_scrmsd\",\n",
    "                         \"rfdiffusion_unfiltered\"] = \"uniref50_202401\"\n",
    "\n",
    "@dataclass\n",
    "class MSAConfig(datasets.BuilderConfig):\n",
    "        '''Congif for MSA generation'''\n",
    "        name: str = \"msa\"\n",
    "        dataset: Literal[\"uniref50_202401\",\n",
    "                         \"gigaref_with_singletons\",\n",
    "                         \"gigaref_no_singletons\"] = \"uniref50_202401\" #TODO: complete all possible datasets\n",
    "\n",
    "# Name of the dataset usually matches the script name with CamelCase instead of snake_case\n",
    "class DayhoffDataset(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "    \n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "    DEFAULT_CONFIG_NAME = \"sequence\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        SequencesConfig(version=VERSION, description=\"sequence datasets\"),\n",
    "        ClusteredSequencesConfig(version=VERSION, description=\"Clustered sequence datasets\"),\n",
    "        MSAConfig(version=VERSION, description=\"MSA datasets\"),\n",
    "\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
    "        # Maybe add sequence and MSA configs?\n",
    "\n",
    "        homepage= \"\" # TODO: add HF homepage\n",
    "\n",
    "        if self.config.name == \"sequence\":\n",
    "            features = datasets.Features(\n",
    "                {   \"index\": datasets.Value(\"int32\"),\n",
    "                    \"accession\": datasets.Value(\"string\"),\n",
    "                    \"sequence\": datasets.Value(\"large_string\"),\n",
    "                    \"description\": datasets.Value(\"string\")\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        elif self.config.name == \"clustered\":\n",
    "            features = datasets.Features(\n",
    "                {   \"indexes\": Sequence(datasets.Value(\"int32\")),\n",
    "                    \"accessions\": Sequence(datasets.Value(\"string\")),\n",
    "                    \"sequences\": Sequence(datasets.Value(\"large_string\")),\n",
    "                    \"descriptions\": Sequence(datasets.Value(\"string\"))\n",
    "                  \n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "        else:\n",
    "             raise NotImplementedError(f\"{self.config.name} config not implemented yet\") #TODO: Implement msa config\n",
    "\n",
    "        return datasets.DatasetInfo(\n",
    "            features=features,  \n",
    "            license=_LICENSE,\n",
    "            citation=_CITATION,        \n",
    "            supervised_keys=None\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        # This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        DATASETS_WITHOUT_SPLITS = [\"rfdiffusion_both_filter\",\n",
    "                                   \"rfdiffusion_novelty\",\n",
    "                                   \"rfdiffusion_scrmsd\",\n",
    "                                   \"rfdiffusion_unfiltered\"]\n",
    "        \n",
    "        if self.config.name == \"sequence\" and (self.config.dataset not in DATASETS_WITHOUT_SPLITS):\n",
    "\n",
    "            \n",
    "            file_paths = dl_manager.download({\n",
    "                 'consensus':hf_hub_url(repo_id=_REPO_ID, filename=\"consensus_sample.fasta\",subfolder=self.config.dataset, repo_type='dataset'), #TODO: using _sample for now\n",
    "                 'splits':hf_hub_url(repo_id=_REPO_ID, filename=\"sample_splits.json\",subfolder=self.config.dataset,repo_type='dataset')\n",
    "            })\n",
    "\n",
    "            fasta_path = file_paths['consensus']\n",
    "            splits_path = file_paths['splits']\n",
    "\n",
    "            return [\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TRAIN,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"train\"\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.VALIDATION,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"valid\"\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TEST,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"test\"\n",
    "                    },\n",
    "                ),\n",
    "            ]\n",
    "        elif self.config.name == \"sequence\" and (self.config.dataset in DATASETS_WITHOUT_SPLITS):\n",
    "\n",
    "            if \"rfdiffusion\" in self.config.dataset:\n",
    "                subfolder = \"rfdiffusion\"\n",
    "\n",
    "            file_paths = dl_manager.download({\n",
    "                 'data':hf_hub_url(repo_id=_REPO_ID, filename=f\"{self.config.dataset}.fasta\",subfolder = subfolder, repo_type='dataset'), #TODO: using _sample for now\n",
    "            })\n",
    "\n",
    "            fasta_path = file_paths['data']\n",
    "\n",
    "            return [\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TRAIN,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\": None,\n",
    "                        \"split\": \"train\"\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        elif self.config.name == \"clustered\":\n",
    "            \n",
    "            file_paths = dl_manager.download({\n",
    "                 'consensus':hf_hub_url(repo_id=_REPO_ID, filename=\"consensus_sample.fasta\",subfolder=self.config.dataset, repo_type='dataset'), #TODO: using _sample for now\n",
    "                 'splits':hf_hub_url(repo_id=_REPO_ID, filename=\"sample_clustered_splits.json\",subfolder=self.config.dataset,repo_type='dataset')\n",
    "            })\n",
    "\n",
    "            fasta_path = file_paths['consensus']\n",
    "            splits_path = file_paths['splits']\n",
    "\n",
    "            return [\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TRAIN,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"train\"\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.VALIDATION,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"valid\"\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TEST,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"fasta_path\": fasta_path,\n",
    "                        \"splits_path\":splits_path,\n",
    "                        \"split\": \"test\"\n",
    "                    },\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    def _generate_examples(self, fasta_path, splits_path, split):\n",
    "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "\n",
    "        \n",
    "        if self.config.name == \"sequence\":        \n",
    "\n",
    "            dataset = Dataset.from_generator(\n",
    "                parse_pyfastx_generator,\n",
    "                gen_kwargs={\n",
    "                    \"fasta_fpath\": fasta_path\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if splits_path is not None:\n",
    "                with open(splits_path,'r') as f:\n",
    "                    splits = json.load(f)\n",
    "\n",
    "                dataset = dataset.select(splits[split])\n",
    "        \n",
    "            for key,data in enumerate(dataset):\n",
    "                yield key,data\n",
    "\n",
    "\n",
    "        if self.config.name == \"clustered\":\n",
    "\n",
    "            with open(splits_path,'r') as f:\n",
    "                splits = json.load(f)\n",
    "\n",
    "            dataset = Dataset.from_generator(\n",
    "                parse_pyfastx_generator,\n",
    "                gen_kwargs={\n",
    "                    \"fasta_fpath\": fasta_path\n",
    "                }\n",
    "            )\n",
    "\n",
    "            dataset = make_dset_from_ids(\n",
    "                ids = splits[split],\n",
    "                seq_dset = dataset\n",
    "            ).rename_columns({\n",
    "                \"index\":\"indexes\",\n",
    "                \"accession\":\"accessions\",\n",
    "                \"sequence\":\"sequences\",\n",
    "                \"description\":\"descriptions\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            for key,data in enumerate(dataset):\n",
    "                yield key,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DownloadManager\n",
    "\n",
    "# dl_manager = DownloadManager()\n",
    "# dl_manager.dow('https://huggingface.co/datasets/samirchar/DayhoffDataset/resolve/main/uniref50_202401/parquets/train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_hub_url(repo_id=_REPO_ID, filename=\"parquets/\",subfolder='uniref50_202401', repo_type='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_manager.download({\n",
    "                 'consensus':hf_hub_url(repo_id=_REPO_ID, filename=\"consensus_sample.fasta\",subfolder=self.config.dataset, repo_type='dataset'), #TODO: using _sample for now\n",
    "                 'splits':hf_hub_url(repo_id=_REPO_ID, filename=\"splits.json\",subfolder=self.config.dataset,repo_type='dataset'),\n",
    "                 'lengths_and_offsets':hf_hub_url(repo_id=_REPO_ID, filename=\"lengths_and_offsets.npz\",subfolder=self.config.dataset,repo_type='dataset')\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of files in local dir\n",
    "num_local_files = 0\n",
    "local_dirs = ['../data/gigaref_full/with_singletons/']\n",
    "\n",
    "local_file_names = []\n",
    "for local_dir in local_dirs:\n",
    "    files = glob(osp.join(local_dir, \"**\"),recursive=True)\n",
    "    num_local_files += len(files)\n",
    "    # append the path\n",
    "    local_file_names.extend([file.replace('data/','') for file in files if osp.isfile(file)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "token = os.environ.get(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gigaref_singletons = load_dataset(\"microsoft/DayhoffDataset\",\n",
    "                  name=\"gigaref_only_singletons\",\n",
    "                  token=token,\n",
    "                  streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(ds_gigaref_singletons[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"microsoft/DayhoffDataset\",\n",
    "                  name=\"rfdiffusion\",\n",
    "                  split = \"rfdiffusion_both_filter\",\n",
    "                  token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_uniref90_test = load_dataset(\"microsoft/DayhoffDataset\",\n",
    "                  name=\"uniref90\",\n",
    "                #   split = \"test\",\n",
    "                  token=token,\n",
    "                  streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ds_uniref90_test[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_uniref50_test = load_dataset(\"microsoft/DayhoffDataset\",\n",
    "                  name=\"uniref50\",\n",
    "                #   split = \"test\",\n",
    "                  token=token,\n",
    "                  streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ds_uniref50_test[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_json(\"../data/rfdiffusion/jsonl/rfdiffusion_both_filter.jsonl.gz\",lines=True,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"samirchar/DayhoffDataset\",\n",
    "                  \"sequence\",\n",
    "                  dataset=\"rfdiffusion_both_filter\",\n",
    "                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk(\"../data/rfdiffusion/rfdiffusion_both_filter_arrows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_loader = DataLoader(a,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a_loader:\n",
    "    print(i['sequence'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/uniref50_202401/splits.json','r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "offsets = np.load('../data/uniref50_202401/lengths_and_offsets.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/uniref50_202401/splits.json','r') as f:\n",
    "    splits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dayhoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
