{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, Sequence, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP\n",
    ")\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sequence_models.samplers import (\n",
    "    SortishSampler,\n",
    "    ApproxBatchSampler,\n",
    "    ClusteredSortishSampler,\n",
    ")\n",
    "\n",
    "from evodiff.utils import Tokenizer\n",
    "from dayhoff.collators import LMCollator, OAMaskCollator\n",
    "from dayhoff.constants import MSA_ALPHABET_PLUS, TaskType\n",
    "from dayhoff.datasets import UniRefDataset\n",
    "from dayhoff.model import (\n",
    "    ARDiffusionModel,\n",
    "    OrderAgnosticDiffusionModel,\n",
    ")\n",
    "from dayhoff.model import create_model\n",
    "\n",
    "\n",
    "# default to a single-GPU setup if not present\n",
    "RANK = 0\n",
    "LOCAL_RANK = 0\n",
    "WORLD_SIZE = 1\n",
    "DEVICE = torch.device(f\"cuda:0\")\n",
    "\n",
    "\n",
    "def is_amlt() -> bool:\n",
    "    return os.environ.get(\"AMLT_OUTPUT_DIR\", None) is not None\n",
    "\n",
    "\n",
    "def load_config_and_model(\n",
    "    config_fpath: str,\n",
    ") -> Tuple[dict, Tokenizer, nn.Module, Type[nn.Module]]:\n",
    "    \"\"\"Parses the experiment config to load the model and tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config_fpath: str\n",
    "        The path to the experiment config file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    config: dict\n",
    "        The experiment config\n",
    "    tokenizer: Tokenizer\n",
    "        The model's tokenizer\n",
    "    model: nn.Module\n",
    "        A task-wrapped version of the specified model, which returns the appropriate loss and metrics\n",
    "    block: Type[nn.Module]\n",
    "        The block class used repeatedly in the module. It should not be split by any sharding.\n",
    "    \"\"\"\n",
    "    with open(config_fpath, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    config[\"task\"] = config[\"task\"].lower().strip()\n",
    "    tokenizer = Tokenizer(MSA_ALPHABET_PLUS)\n",
    "    task = TaskType(config[\"task\"].lower().strip())\n",
    "\n",
    "    # create the model\n",
    "    model, block = create_model(\n",
    "        task, config[\"model_type\"], config[\"model_config\"], tokenizer.mask_id.item()\n",
    "    )\n",
    "\n",
    "    # add the task-specific wrapper\n",
    "    aux_loss_weight = config.get(\"aux_loss_weight\", 0.0)\n",
    "    if task == TaskType.OADM:\n",
    "        model = OrderAgnosticDiffusionModel(\n",
    "            model, tokenizer.pad_id, aux_loss_weight=aux_loss_weight\n",
    "        )\n",
    "    elif task == TaskType.LM:\n",
    "        model = ARDiffusionModel(model, aux_loss_weight=aux_loss_weight)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {config['task']}\")\n",
    "    return config, tokenizer, model, block\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    config: dict, tokenizer: Tokenizer, args: argparse.Namespace\n",
    ") -> DataLoader:\n",
    "    if is_amlt():\n",
    "        data_top_dir = args.data_root or \"/ddn/evodiff/\"\n",
    "    else:\n",
    "        data_top_dir = args.data_root or \"/data1/data/\"\n",
    "\n",
    "    dataset = config[\"dataset\"]\n",
    "    data_dir = os.path.join(data_top_dir, dataset + \"/\")\n",
    "\n",
    "    if config[\"task\"] == \"oadm\":\n",
    "        collator = OAMaskCollator(\n",
    "            tokenizer=tokenizer,\n",
    "            pad_to_multiple_of=config.get(\"pad_to_multiple_of\", None),\n",
    "        )\n",
    "    elif config[\"task\"] == \"lm\":\n",
    "        collator = LMCollator(\n",
    "            tokenizer=tokenizer,\n",
    "            pad_to_multiple_of=config.get(\"pad_to_multiple_of\", None),\n",
    "            flip_prob=config.get(\"flip_prob\", 0.0),\n",
    "            fim_prob=config.get(\"fim_prob\", 0.0),\n",
    "            swap_bos_eos_on_flip=config.get(\"swap_bos_eos_on_flip\", True),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {config['task']}\")\n",
    "\n",
    "    # load the dataset\n",
    "    ds_train = UniRefDataset(data_dir, \"train\", max_len=config[\"max_len\"])\n",
    "    train_idx = ds_train.indices\n",
    "\n",
    "    # create the dataloader\n",
    "    if args.mini_run:\n",
    "        tindices = np.arange(\n",
    "            0, 1000\n",
    "        )  # np.arange(21546293,31546293,1)#(1000000,21546293, 1)\n",
    "        train_indices = np.sort(np.random.choice(tindices, 100, replace=False))\n",
    "        train_sampler = Subset(ds_train, train_indices)\n",
    "        len_train = train_indices\n",
    "        dl_train = DataLoader(\n",
    "            dataset=train_sampler,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "            num_workers=1,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    else:\n",
    "        metadata = np.load(os.path.join(data_dir, \"lengths_and_offsets.npz\"))\n",
    "        len_train = np.minimum(metadata[\"ells\"][train_idx], config[\"max_len\"])\n",
    "        if \"uniref50\" in dataset:\n",
    "            train_sortish_sampler = SortishSampler(\n",
    "                len_train, config[\"bucket_size\"], num_replicas=WORLD_SIZE, rank=RANK\n",
    "            )\n",
    "        elif \"uniref90\" in dataset:\n",
    "            with open(os.path.join(data_dir) + \"clustered_splits.json\") as f:\n",
    "                clusters = json.load(f)[\"train\"]\n",
    "            train_sortish_sampler = ClusteredSortishSampler(\n",
    "                len_train,\n",
    "                clusters,\n",
    "                config[\"bucket_size\"],\n",
    "                num_replicas=WORLD_SIZE,\n",
    "                rank=RANK,\n",
    "            )\n",
    "        train_sampler = ApproxBatchSampler(\n",
    "            train_sortish_sampler,\n",
    "            config[\"max_tokens\"],\n",
    "            config[\"max_batch_size\"],\n",
    "            len_train,\n",
    "            batch_mult=8,\n",
    "        )\n",
    "\n",
    "        dl_train = DataLoader(\n",
    "            dataset=ds_train,\n",
    "            batch_sampler=train_sampler,\n",
    "            num_workers=8,\n",
    "            collate_fn=collator,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    return dl_train\n",
    "\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_fpath\")\n",
    "parser.add_argument(\n",
    "    \"out_fpath\",\n",
    "    type=str,\n",
    "    nargs=\"?\",\n",
    "    default=os.getenv(\"AMLT_OUTPUT_DIR\", \"/tmp\") + \"/\",\n",
    ")\n",
    "parser.add_argument(\"data_root\", type=str, nargs=\"?\", default=None)\n",
    "parser.add_argument(\n",
    "    \"--mini_run\", action=\"store_true\"\n",
    ")  # Set to True if running on subset of data\n",
    "parser.add_argument(\"--checkpoint_freq\", type=int, default=2000)  # in steps\n",
    "parser.add_argument(\n",
    "    \"--random_seed\", type=int, default=0\n",
    ")  # lambda reweighting term from Austin D3PM\n",
    "parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")\n",
    "parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "parser.add_argument(\"--no_wandb\", action=\"store_true\")\n",
    "parser.add_argument(\"--last_step\", default=-1, type=int)\n",
    "\n",
    "fake_args = [\n",
    "    \"jamba3B-uniref50.json\",       # config_fpath\n",
    "    \"/tmp/output/\",        # out_fpath\n",
    "    \"../data/\",  # data_root\n",
    "    # \"--mini_run\",\n",
    "    \"--no_wandb\"\n",
    "]\n",
    "\n",
    "args = parser.parse_args(fake_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job on rank 0 with local rank 0 and world size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirchar/miniconda3/envs/dayhoff/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 30 as padding index\n",
      "Using 28 as masking index\n",
      "Model has 2981431616 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Starting job on rank {RANK} with local rank {LOCAL_RANK} and world size {WORLD_SIZE}\"\n",
    ")\n",
    "seed_everything(args.random_seed)\n",
    "\n",
    "# dist.init_process_group(backend=\"nccl\")\n",
    "# get the config, tokenizer, and model\n",
    "if args.verbose:\n",
    "    print(\"Initializing model...\", RANK)\n",
    "config, tokenizer, model, blk_types = load_config_and_model(args.config_fpath)\n",
    "if RANK == 0:\n",
    "    if args.no_wandb:\n",
    "        wandbmode = \"disabled\"\n",
    "    else:\n",
    "        wandbmode = \"online\"\n",
    "    wandb.init(config=config, mode=wandbmode)\n",
    "if args.verbose:\n",
    "    print(\"Done initializing model.\", RANK)\n",
    "\n",
    "# store the command line args in the config and dump to disk\n",
    "config[\"dtype\"] = args.dtype\n",
    "config[\"random_seed\"] = args.random_seed\n",
    "config[\"world_size\"] = WORLD_SIZE\n",
    "if RANK == 0:\n",
    "    os.makedirs(args.out_fpath, exist_ok=True)\n",
    "    with open(os.path.join(args.out_fpath, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "# training dtype and local device\n",
    "dtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "}[args.dtype]\n",
    "\n",
    "padding_idx = tokenizer.pad_id  # PROTEIN_ALPHABET.index(PAD)\n",
    "if RANK == 0:\n",
    "    print(\"Using {} as padding index\".format(padding_idx))\n",
    "    print(\"Using {} as masking index\".format(tokenizer.mask_id))\n",
    "    print(\n",
    "        f\"Model has {sum(p.numel() for p in model.parameters())} trainable parameters.\"\n",
    "    )\n",
    "if args.verbose:\n",
    "    print(\"Moving and sharding model...\", RANK)\n",
    "# set the default device\n",
    "torch.cuda.set_device(LOCAL_RANK)\n",
    "\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Initializing data...\", RANK)\n",
    "dl_train = get_dataloader(config, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Documentation create dataset loading script: https://huggingface.co/docs/datasets/en/dataset_script\n",
    "# Basic dataset creation and benefits of using HF datasets (first paragraph): https://huggingface.co/docs/datasets/en/create_dataset\n",
    "# Another good (Japanese) tutorial for create_dataset: https://zenn.dev/mjun0812/articles/cfe4b7346ba6b4\n",
    "\n",
    "from huggingface_hub import login, ModelCard\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter = iter(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter_obs = next(dl_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 80]), torch.Size([1000, 80]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_train_iter_obs[0].shape,dl_train_iter_obs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_iter_obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([4, 1, 3, 3]), 'text': [\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\", 'Got a letter in the mail last week that said Dr. Goldberg is moving to Arizona to take a new position there in June.  He will be missed very much.  \\\\n\\\\nI think finding a new doctor in NYC that you actually like might almost be as awful as trying to find a date!']}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os.path as osp\n",
    "from torch.utils.data import Dataset\n",
    "from dayhoff.datasets import UniRefDataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "Dayhoff dataset\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add a link to an official homepage for the dataset here\n",
    "_HOMEPAGE = \"https://github.com/microsoft/dayhoff/tree/main\"\n",
    "\n",
    "# TODO: Add the licence for the dataset here if you can find it\n",
    "_LICENSE = \"\"\n",
    "\n",
    "#TODO: Add citation\n",
    "_CITATION = \"\"\n",
    "\n",
    "# TODO: Add link to the official dataset URLs here\n",
    "# The HuggingFace Datasets library doesn't host the datasets but only points to the original files.\n",
    "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\n",
    "_URLS = {\n",
    "    \n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class SequencesConfig(datasets.BuilderConfig):\n",
    "        '''Congif for sequence generation'''\n",
    "        dataset: Literal[\"uniref50_202401\", \"uniref90_202401\", \"gigaref\"]\n",
    "        name: str = \"sequence\"\n",
    "        max_seq_len: int = 2048\n",
    "\n",
    "@dataclass\n",
    "class MSAConfig(datasets.BuilderConfig):\n",
    "        '''Congif for MSA generation'''\n",
    "        dataset: Literal[\"uniref50_202401\", \"gigaref_with_singletons\", \"gigaref_no_singletons\"] #TODO: complete all possible datasets\n",
    "        name: str = \"msa\"\n",
    "        #TODO: Complete all arguments by looking at dataset class, possibly OpenProteinDataset\n",
    "\n",
    "\n",
    "# Name of the dataset usually matches the script name with CamelCase instead of snake_case\n",
    "class Dayhoff(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "    \n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "    DEFAULT_CONFIG_NAME = \"sequence\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        SequencesConfig(version=VERSION, description=\"sequence generation\"),\n",
    "        MSAConfig(version=VERSION, description=\"MSA generation\"),\n",
    "\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    def _info(self):\n",
    "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
    "        # Maybe add sequence and MSA configs?\n",
    "        if self.config.name == \"sequence\":\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"sequence\": datasets.Value(\"string\")\n",
    "                }\n",
    "            )\n",
    "\n",
    "            homepage= \"\" # TODO: add HF homepage\n",
    "\n",
    "        if self.config.name == \"msa\":\n",
    "             raise NotImplementedError(\"MSA config not implemented yet\") #TODO: Implement MSA config\n",
    "\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            # description=_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,  \n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "            \n",
    "            supervised_keys=None\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        if self.config.name == \"sequence\":\n",
    "            \n",
    "            file_paths = dl_manager.download({\n",
    "                 'consensus':'https://huggingface.co/datasets/samirchar/dayhoff/consensus_sample.fasta',\n",
    "                 'splits':'https://huggingface.co/datasets/samirchar/dayhoff/splits.json',\n",
    "                 'splits':'https://huggingface.co/datasets/samirchar/dayhoff/splits_and_offsets.npz'\n",
    "            })\n",
    "\n",
    "            return [\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TRAIN,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"data_dir\": \"\", #TODO: Add data dir\n",
    "                        \"split\": \"train\",\n",
    "                        \"max_len\":self.config.max_seq_len, #TODO: can max_len and split be passed as arguments?\n",
    "                        \"split_file\":None\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.VALIDATION,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"data_dir\": \"\", #TODO: Add data dir\n",
    "                        \"split\": \"valid\",\n",
    "                        \"max_len\":self.config.max_seq_len, #TODO: can max_len and split be passed as arguments?\n",
    "                        \"split_file\":None\n",
    "                    },\n",
    "                ),\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TEST,\n",
    "                    # These kwargs will be passed to _generate_examples\n",
    "                    gen_kwargs={\n",
    "                        \"data_dir\": \"\", #TODO: Add data dir\n",
    "                        \"split\": \"test\",\n",
    "                        \"max_len\":self.config.max_seq_len, #TODO: can max_len and split be passed as arguments?\n",
    "                        \"split_file\":None\n",
    "                    },\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    def _generate_examples(self, data_dir, split, max_len, split_file):\n",
    "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "\n",
    "        if self.config.name == \"sequence\":\n",
    "            dataset = UniRefDataset(data_dir = data_dir, split = split, max_len = max_len, split_file = split_file)\n",
    "            for key, row in enumerate(dataset):\n",
    "                yield key, {\n",
    "                    \"sequence\": row\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/uniref50_202401/splits.json','r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "offsets = np.load('../data/uniref50_202401/lengths_and_offsets.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([          0,       45467,       85257, ..., 25814319433,\n",
       "       25814319555, 25814319685])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets['name_offsets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/uniref50_202401/splits.json','r') as f:\n",
    "    splits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'valid', 'rtest'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dayhoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
